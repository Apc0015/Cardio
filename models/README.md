# Models Directory

This directory contains all trained machine learning models and preprocessing objects.

## Structure

```
models/
 baseline_models/ # Traditional ML models
 advanced_models/ # Deep learning models
 preprocessing/ # Scalers, encoders, transformers
```

## Subdirectories

### `baseline_models/`
Traditional machine learning models trained on the dataset.

**Expected files:**
- `logistic_regression.pkl` - Logistic Regression model
- `decision_tree.pkl` - Decision Tree Classifier
- `random_forest.pkl` - Random Forest Classifier

**Generated by:** `notebooks/baseline_models.ipynb`

### `advanced_models/`
Advanced machine learning and deep learning models.

**Expected files:**
- `xgboost.pkl` - XGBoost Classifier
- `gradient_boosting.pkl` - Gradient Boosting Classifier
- `neural_network.h5` - Neural Network (Keras/TensorFlow)
- `ensemble.pkl` - Hybrid ensemble model

**Generated by:** `notebooks/advanced_models.ipynb`

### `preprocessing/`
Preprocessing objects used for data transformation.

**Expected files:**
- `scaler.pkl` - Feature scaler (StandardScaler/MinMaxScaler)
- `label_encoder.pkl` - Target variable encoder
- `preprocessing_metadata.txt` - Documentation of preprocessing steps

**Generated by:** `notebooks/data_preprocessing.ipynb`

## Model Pipeline

```
Raw Patient Data
 |
[Preprocessing Objects]
 |
Scaled/Encoded Features
 |
[Baseline Models] [Advanced Models]
 | |
Individual Predictions -> [Ensemble] -> Final Prediction
```

## Loading Models

### Python API

```python
from src.utils.model_utils import ModelPredictor

# Initialize predictor
predictor = ModelPredictor('models/')

# Load all models
predictor.load_models()

# Make prediction
result = predictor.predict(input_data)
```

### Manual Loading

```python
import joblib
import pickle

# Load a specific model
model = joblib.load('models/baseline_models/random_forest.pkl')

# Load preprocessing objects
scaler = joblib.load('models/preprocessing/scaler.pkl')
```

## Model Performance

| Model | Accuracy | ROC-AUC | F1-Score |
|-------|----------|---------|----------|
| Decision Tree | 87.76% | 95.19% | 87.43% |
| Random Forest | 84.08% | 92.47% | 84.85% |
| XGBoost | ~90% | ~94% | ~89% |
| Ensemble | **92%+** | **95%+** | **90%+** |

## Training Models

1. **Preprocess Data First:**
 ```bash
 jupyter notebook notebooks/data_preprocessing.ipynb
 ```

2. **Train Baseline Models:**
 ```bash
 jupyter notebook notebooks/baseline_models.ipynb
 ```

3. **Train Advanced Models:**
 ```bash
 jupyter notebook notebooks/advanced_models.ipynb
 ```

## Important Notes

- Model files (`.pkl`, `.h5`) are git-ignored
- Large binary files excluded from version control
- Must train models after cloning repository
- Models are platform-specific (train on target OS)
- Keep track of scikit-learn/TensorFlow versions

## Model Versioning

Create a `models/VERSION.txt` file to track model versions:

```
Model Version: 1.0.0
Training Date: 2024-11-19
Dataset Version: 1.0.0
scikit-learn: 1.3.0
xgboost: 2.0.0
tensorflow: 2.15.0

Performance:
- Ensemble ROC-AUC: 95.2%
- Test Accuracy: 92.1%
```

## Backup Models

For production deployments, backup models to cloud storage:

```bash
# AWS S3
aws s3 cp models/ s3://your-bucket/cardiofusion/models/ --recursive

# Azure Blob Storage
az storage blob upload-batch -d cardiofusion-models -s models/

# Google Cloud Storage
gsutil -m cp -r models/ gs://your-bucket/cardiofusion/
```

## Re-training Schedule

- **Monthly:** Evaluate performance metrics
- **Quarterly:** Retrain with new data (if available)
- **Annually:** Full model architecture review

---

For model usage, see [API.md](../docs/API.md)
